{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zX4Kg8DUTKWO"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://drive.google.com/open?id=1Cq6Yg53UaL0OLDnAz5n2ZFGwv9TeHVGT\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "P-AhVYeBWgQ3",
    "outputId": "04dbbd34-43a4-44bc-c108-1bf282ec7ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "#!pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IoM4VFxWpMR"
   },
   "outputs": [],
   "source": [
    "#importing tensorflow datasets\n",
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHQ2Ko0zl7M4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(str(s.numpy()))\n",
    "  training_labels.append(l.numpy())\n",
    "  \n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(str(s.numpy()))\n",
    "  testing_labels.append(l.numpy())\n",
    "  \n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "Y9O0vkIjEfFI",
    "outputId": "a73c468d-32ab-4261-f7f2-d6d6ff8e53c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1]\n",
      "['b\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"', \"b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\", \"b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'\", \"b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.'\", 'b\\'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.\\'']\n"
     ]
    }
   ],
   "source": [
    "print(training_labels_final[:5])\n",
    "print(training_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n15yyMdmoH1"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000 #num of words in model vocabulary\n",
    "embedding_dim = 16 #embedding dimension\n",
    "max_length = 120 #max num of words in a sentence, short sentence -> padded, long sent -> truncated\n",
    "trunc_type='post' #truncating words from rear\n",
    "oov_tok = \"<OOV>\" #marking out of vocabulary words\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "#{'word': 'index' for all words in vocab}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#converting sentences into seq of numbers\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "#padding or truncating for achieving same length\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "#same thing on test dataset\n",
    "#testing sequences are tokenized based on word_index that was learned from the training words\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "htwg17AdFJBk",
    "outputId": "c4cff9c0-1e67-4fd3-8e55-a66306adce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV> photographs the <OOV> rocky mountains in a superb fashion and jimmy stewart and walter brennan give enjoyable performances as they always seem to do br br but come on hollywood a <OOV> telling the people of dawson city <OOV> to <OOV> themselves a <OOV> yes a <OOV> and to <OOV> the law themselves then <OOV> battling it out on the streets for control of the town br br nothing even remotely resembling that happened on the canadian side of the border during the <OOV> gold rush mr mann and company appear to have mistaken dawson city for <OOV> the canadian north for the american wild west br br canadian viewers be prepared for a <OOV> madness type of enjoyable\n",
      "[   1 6175    2    1 4916 4029    9    4  912 1622    3 1969 1307    3\n",
      " 2384 8836  201  746  361   15   34  208  308    6   83    8    8   19\n",
      "  214   22  352    4    1  990    2   82    5 3608  545    1    6    1\n",
      "  539    4    1  434    4    1    3    6    1    2 1176  539   95    1\n",
      " 8111   10   46   22    2 1996   16 1153    5    2  511    8    8  163\n",
      "   62 2624 7315   13  586   22    2 2297  507    5    2 3652  317    2\n",
      "    1 1835 3445  451 4030    3 1168  985    6   28 4091 3608  545   16\n",
      "    1    2 2297 2430   16    2  299 1357 1259    8    8 2297  803   29\n",
      " 2871   16    4    1 3028  564    5  746]\n"
     ]
    }
   ],
   "source": [
    "#Testing cell, run the cell below first\n",
    "text = padded[2]\n",
    "ans = ' '.join([reverse_word_index.get(i, '???') for i in text])\n",
    "print(ans)\n",
    "#print(reverse_word_index)\n",
    "print(padded[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "9axf0uIXVMhO",
    "outputId": "c64a39da-cf87-4d2f-e2ce-41642cf8dc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?? ?? b this was an absolutely terrible movie don't be <OOV> in by christopher walken or michael <OOV> both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the <OOV> rebels were making their cases for <OOV> maria <OOV> <OOV> appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining <OOV> like christopher <OOV> good name i could barely sit through it\n",
      "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    }
   ],
   "source": [
    "#for decoding a review from padded sequence to text\n",
    "#reverse_word_index = {'index': 'word'}\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "def decode_review(text):\n",
    "  #replacing index with words in vocabulary, replacing with \"??\" if word is not found\n",
    "  return ' '.join([reverse_word_index.get(i, '??') for i in text])\n",
    "\n",
    "print(decode_review(padded[0]))\n",
    "print(training_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "5NEpdhb8AxID",
    "outputId": "6a4e0bb8-798f-404b-8b52-ad103e9023cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Neural network model, top layer embedding, output of embedding layer will be flattened, then a layer of 6 neurons & the output layer consists of 1 neuron\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "V5LLrXC-uNX6",
    "outputId": "da93f5be-3ddf-4f66-df41-97fa30d63ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 9.6292e-05 - accuracy: 1.0000 - val_loss: 0.8403 - val_accuracy: 0.8282\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 5.9596e-05 - accuracy: 1.0000 - val_loss: 0.8753 - val_accuracy: 0.8278\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 3.7025e-05 - accuracy: 1.0000 - val_loss: 0.9094 - val_accuracy: 0.8281\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 2.3464e-05 - accuracy: 1.0000 - val_loss: 0.9481 - val_accuracy: 0.8272\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 1.4823e-05 - accuracy: 1.0000 - val_loss: 0.9812 - val_accuracy: 0.8278\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 9.3033e-06 - accuracy: 1.0000 - val_loss: 1.0119 - val_accuracy: 0.8282\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 5.9994e-06 - accuracy: 1.0000 - val_loss: 1.0485 - val_accuracy: 0.8278\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 3.8615e-06 - accuracy: 1.0000 - val_loss: 1.0842 - val_accuracy: 0.8276\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 2.4528e-06 - accuracy: 1.0000 - val_loss: 1.1161 - val_accuracy: 0.8277\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.5743e-06 - accuracy: 1.0000 - val_loss: 1.1511 - val_accuracy: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efbad299160>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training for 10 epochs\n",
    "#this model is overfitted......\n",
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "H-RRGNQbhhXS",
    "outputId": "14031dff-5606-4c4d-b36f-c4d0aee82d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4394315e-01]\n",
      " [9.9601620e-01]\n",
      " [1.6095247e-07]\n",
      " [1.6940733e-06]\n",
      " [9.9996567e-01]]\n",
      "[1 1 0 0 1]\n",
      "[[9.4601192e-16]\n",
      " [4.4422499e-08]\n",
      " [3.0696822e-06]\n",
      " [1.0000000e+00]\n",
      " [9.9999917e-01]]\n",
      "[0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "\n",
    "#on test data\n",
    "print(model.predict(testing_padded[:5]))\n",
    "print(testing_labels_final[:5])\n",
    "\n",
    "#on training data\n",
    "print(model.predict(padded[:5]))\n",
    "print(training_labels_final[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "yAmjJqEyCOF_",
    "outputId": "1b84e9cb-c204-4396-bf9d-2e26e4b40089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4394315e-01]\n",
      " [9.9601620e-01]\n",
      " [1.6095247e-07]\n",
      " [1.6940733e-06]\n",
      " [9.9996567e-01]]\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#generating vecs.tsv & meta.tsv for projecting on http://projector.tensorflow.org/\n",
    "\n",
    "# e = output of embedding\n",
    "e = model.layers[0]\n",
    "\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmB0Uxk0ycP6"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "#saving the words on meta.tsv & weights on vecs.tsv for projection\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDeqpOCVydtq"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')\n",
    "\n",
    "#go to http://projector.tensorflow.org/ , upload these two files which will allow you to plot these 16 dimensional vectors in a 3 dimensional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "YRxoxc2apscY",
    "outputId": "093bcee2-b385-4dac-82a1-b0455f482cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11], [], [1431], [966], [4], [1537], [1537], [4715], [], [790], [2019], [11], [2929], [2184], [], [790], [2019], [11], [579], [], [11], [579], [], [4], [1782], [4], [4517], [11], [2929], [1275], [], [], [2019], [1003], [2929], [966], [579], [790], []]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I really think this is amazing. honest.\"\n",
    "sequence = tokenizer.texts_to_sequences(sentence)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhgUoW4uBtXf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Course 3 - Week 2 - Lesson 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
